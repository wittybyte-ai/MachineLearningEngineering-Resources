# MachineLearningEngineering-Resources


## Week 1:

- Learn ML
    
    [Getting Started with LLMs ](https://www.notion.so/Getting-Started-with-LLMs-e5043cbc1678464d9e2f520628646e7a?pvs=21)
    
- System Design:
    
    [**Design a News Feed Recommendation Engine**](https://www.notion.so/Design-a-News-Feed-Recommendation-Engine-0f54b00a26a04a5a9fc38b1d0c3dfe33?pvs=21)
    
    [Surge Pricing for Ride Sharing apps](https://www.notion.so/Surge-Pricing-for-Ride-Sharing-apps-d7355605f1184ce79c01df38b57fd392?pvs=21)
    

## Research Papers (Do give them a good read):

The field of Machine Learning and AI is fast moving (atleast that‚Äôs how it has been since the launch of ChatGPT). There is so much incremental developments happening all the time that its very easy to get lost in the sea of research papers. The following list is a very small subset of curated papers which (in cronological order) have played a significant role in brining LLMs to the current stage:

1. Transformers (Where it all began!): [**"Attention Is All You Need"**](https://www.notion.so/Attention-Is-All-You-Need-d0bc0a93ae5c4ffeb5edcc844cab19aa?pvs=21) 
2. GPT 1: [Improving Language Understanding by Generative Pre-Training ](https://www.notion.so/Improving-Language-Understanding-by-Generative-Pre-Training-ff61273ec6aa4683a5be5687b6ffa5d9?pvs=21) 
3. GPT 2: [Language Models are Unsupervised Multitask Learners](https://www.notion.so/Language-Models-are-Unsupervised-Multitask-Learners-9d045761b7784c4abcc15296ee68a3c0?pvs=21) 
4. GPT 3: [Language Models are Few-Shot Learners](https://www.notion.so/Language-Models-are-Few-Shot-Learners-c4d70415440745c3888a4f64418112e4?pvs=21) 
5. Codex (GPT starts to code): [Evaluating Large Language Models Trained on Code](https://www.notion.so/Evaluating-Large-Language-Models-Trained-on-Code-c5c240f88b9d4014afa02fd8499f023c?pvs=21) 
6. LLaMa (Meta‚Äôs Foundation Open Source): [LLaMA: Open and Efficient Foundation Language Models](https://www.notion.so/LLaMA-Open-and-Efficient-Foundation-Language-Models-bec7addf37d0452b828022fbb4b100d6?pvs=21) 
7. LLaMa 2 (Lets democratize LLMs) : [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://www.notion.so/Llama-2-Open-Foundation-and-Fine-Tuned-Chat-Models-adf607ff026441fdb320eeba24f7fb22?pvs=21) 

## **Books (To ground your understanding):**

- *"Pattern Recognition and Machine Learning"* by Bishop: A seminal piece that offers deep insights into pattern recognition, vital for any budding ML engineer.
- *"Deep Learning"* by Goodfellow, Bengio, and Courville: As the name suggests, it‚Äôs an essential read for diving into deep learning.
- *"Machine Learning: A Probabilistic Perspective"* by Kevin Murphy: A detailed look into the probabilistic side of ML.
- *"Reinforcement Learning: An Introduction"* by Sutton and Barto: Understand the dynamics of reinforcement learning

## **Some other resources (Just for fun):**

- Instruction finetuning and RLHF lecture [Youtube](https://www.youtube.com/watch?v=zjrM-MW-0y0)
- Open Pretrained Transformers [Youtube](https://www.youtube.com/watch?v=p9IxoSkvZ-M&t=4s)
- How Does ChatGPT Work? [Slides](https://docs.google.com/presentation/d/1TTyePrw-p_xxUbi3rbmBI3QQpSsTI1btaQuAUvvNc8w/edit#slide=id.g206fa25c94c_0_24)
- Let's build GPT: from scratch, in code, spelled out. [Video](https://www.youtube.com/watch?v=kCc8FmEb1nY)|[Code](https://github.com/karpathy/ng-video-lecture)
- [500+ Best AI Tools](https://www.notion.so/500-Best-AI-Tools-e954b36bf688404ababf74a13f98d126?pvs=21)
- [Cohere Summarize Beta](https://txt.cohere.ai/summarize-beta/) - Introducing Cohere Summarize Beta: A New Endpoint for Text Summarization
- [chatgpt-wrapper](https://github.com/mmabrouk/chatgpt-wrapper) - ChatGPT Wrapper is an open-source unofficial Python API and CLI that lets you interact with ChatGPT.

I will continue to add free resources here for the community. Feel free to bookmark this page to keep it handy. ü§üüèª

References:

[**"Attention Is All You Need"**](https://www.notion.so/Attention-Is-All-You-Need-d0bc0a93ae5c4ffeb5edcc844cab19aa?pvs=21)

[Improving Language Understanding by Generative Pre-Training ](https://www.notion.so/Improving-Language-Understanding-by-Generative-Pre-Training-ff61273ec6aa4683a5be5687b6ffa5d9?pvs=21)

[Language Models are Unsupervised Multitask Learners](https://www.notion.so/Language-Models-are-Unsupervised-Multitask-Learners-9d045761b7784c4abcc15296ee68a3c0?pvs=21)

[Language Models are Few-Shot Learners](https://www.notion.so/Language-Models-are-Few-Shot-Learners-c4d70415440745c3888a4f64418112e4?pvs=21)

[Evaluating Large Language Models Trained on Code](https://www.notion.so/Evaluating-Large-Language-Models-Trained-on-Code-c5c240f88b9d4014afa02fd8499f023c?pvs=21)

[LLaMA: Open and Efficient Foundation Language Models](https://www.notion.so/LLaMA-Open-and-Efficient-Foundation-Language-Models-bec7addf37d0452b828022fbb4b100d6?pvs=21)

[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://www.notion.so/Llama-2-Open-Foundation-and-Fine-Tuned-Chat-Models-adf607ff026441fdb320eeba24f7fb22?pvs=21)
